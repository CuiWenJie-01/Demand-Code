import logging
import os, sys
import json
import glob
import sys
import shutil
from pathlib import Path
from sympy import EX
from tqdm import tqdm
from prompt_loader import *
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from collections import defaultdict
import multiprocessing as mp

# åŠ è½½ç¬¬ä¸‰é˜¶æ®µ prompt
current_dir = Path(__file__).parent
config_path = current_dir / "prompts.yaml"

# åˆ›å»ºçº¿ç¨‹å±€éƒ¨å­˜å‚¨ï¼Œé¿å…å¤šçº¿ç¨‹ç¯å¢ƒä¸‹é‡å¤åˆ›å»ºloaderå®ä¾‹
thread_local = threading.local()

def get_thread_loader():
    """è·å–çº¿ç¨‹æœ¬åœ°çš„prompt loaderå®ä¾‹"""
    if not hasattr(thread_local, 'loader'):
        thread_local.loader = PromptLoader(str(config_path))
    return thread_local.loader

# gclä¿®æ”¹ï¼šæ‰€æœ‰xxx.get("å­—æ®µå", "").strip()æ”¹ä¸º(xxx.get("å­—æ®µå") or "").strip()
def extract_qa_text(entry):
    # åˆ¤æ–­æ˜¯å¦æ˜¯å¤šé¢˜å‹
    if "sub_qa" in entry and isinstance(entry["sub_qa"], list):
        # èƒŒæ™¯çŸ¥è¯†
        # ç»“æœæ˜¯ Noneã€ç©ºå­—ç¬¦ä¸² ""ã€æˆ–å…¶ä»–"å‡å€¼"ï¼ˆfalsy valueï¼‰ï¼Œå°±ç”¨ç©ºå­—ç¬¦ä¸² "" ä»£æ›¿ï¼Œç„¶åå†è°ƒç”¨ .strip()
        bg = (entry.get("é¢˜ç›®èƒŒæ™¯çŸ¥è¯†") or "").strip()
        # q éƒ¨åˆ†
        q_parts = []
        if bg:
            q_parts.append(bg)
        for qa in entry["sub_qa"]:
            num = (qa.get("é¢˜ç›®ç¼–å·") or "").strip()
            ques = (qa.get("é¢˜ç›®å†…å®¹") or "").strip()
            full_ques = f"é¢˜ç›®{num}  {ques}" if num else f"{ques}"
            q_parts.append(full_ques)
        q_text = "\n".join(filter(None, q_parts))
        # a éƒ¨åˆ†
        a_parts = []
        for qa in entry["sub_qa"]:
            num = (qa.get("é¢˜ç›®ç¼–å·") or "").strip()
            ans = (qa.get("å¯¹åº”ç­”æ¡ˆ") or "").strip()
            full_ans = f"é¢˜ç›®{num}  {ans}" if num else f"{ans}"
            a_parts.append(full_ans)
        a_text = "\n".join(filter(None, a_parts))
    else:
        # å•é¢˜å‹
        num = (entry.get("é¢˜ç›®ç¼–å·") or "").strip()
        ques = (entry.get("é¢˜ç›®å†…å®¹") or "").strip()
        ans = (entry.get("å¯¹åº”ç­”æ¡ˆ") or "").strip()
        q_parts = []
        a_parts = []
        full_ques = f"é¢˜ç›®{num}  {ques}" if num else f"{ques}"
        full_ans = f"é¢˜ç›®{num}  {ans}" if num else f"{ans}"
        if ques:
            q_parts.append(full_ques)
        if ans:
            a_parts.append(full_ans)
        q_text = "\n".join(filter(None, q_parts))
        a_text = "\n".join(filter(None, a_parts))
    
    return q_text, a_text

def extract_subject_from_source(source_type: str) -> str:
    """ä» source_type å­—ç¬¦ä¸²ä¸­æå–å­¦ç§‘åç§°"""
    # ä½¿ç”¨å­—å…¸æŸ¥æ‰¾æé«˜æ•ˆç‡
    subject_keywords = {
        'è‹±è¯­': 'è‹±è¯­',
        'æ”¿æ²»': 'æ”¿æ²»',
        'é“å¾·ä¸æ³•æ²»': 'æ”¿æ²»',
        'å†å²': 'å†å²',
        'è¯­æ–‡': 'è¯­æ–‡',
        'åœ°ç†': 'åœ°ç†',
        'åœ°åŒ–ç”Ÿç‰©æ•°': 'åœ°ç†',
        'æ–‡ç»¼': 'æ–‡ç»¼',
        'ä¿¡æ¯': 'ä¿¡æ¯ç§‘æŠ€'
    }
    
    for keyword, subject in subject_keywords.items():
        if keyword in source_type:
            return subject
            
    raise ValueError(f"æ— æ³•ä» source_type '{source_type}' ä¸­è¯†åˆ«å­¦ç§‘")

def process_single_line(data, file_path, subject):
    """å¤„ç†å•è¡Œæ•°æ®"""
    try:
        q_text, a_text = extract_qa_text(data)
    except Exception as e:
        logging.error(f"å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯{e}ï¼ŒåŸæ•°æ®å¦‚ä¸‹:{data}")
        return None

    # åŠ¨æ€æ„å»ºç¬¬ä¸‰é˜¶æ®µ prompt
    answer_for_prompt = "æ²¡æœ‰ç­”æ¡ˆ" if 'only_q' in file_path else a_text
    
    # è·å–çº¿ç¨‹æœ¬åœ°loader
    local_loader = get_thread_loader()
    
    try:
        full_prompt = local_loader.build_prompt(
            stage="3_check_availability",
            subject=subject,
            query=q_text,
            answer=answer_for_prompt
        )
    except Exception as e:
        logging.error(f"æ„å»ºpromptæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

    # åˆ›å»ºè¾“å‡ºå¯¹è±¡
    id_info = {
        "original_data": data,
        "source_type": data.get('source_type', ''),
        "section": data.get('section', ''),
        "url": data.get('url', ''),
        "img_path": data.get('img_path', '')
    }

    output_obj = {
        "query": full_prompt,
        "id": id_info,
    }
    
    return json.dumps(output_obj, ensure_ascii=False)

def process_single_file(file_path, subject):
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œè¿”å›å¤„ç†ç»“æœåˆ—è¡¨"""
    results = []
    file_name = os.path.basename(file_path)
    
    # è¯»å–è¾“å…¥æ–‡ä»¶
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue
                    
                try:
                    data = json.loads(line)
                except json.JSONDecodeError as e:
                    logging.error(f"JSONè§£æé”™è¯¯ in {file_name} line {line_num}: {e}")
                    continue
                
                result = process_single_line(data, file_path, subject)
                if result:
                    results.append(result)
    except Exception as e:
        logging.error(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
    return results

def process_folder(input_folder, output_file, max_workers=4):
    """å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶å¹¶åˆå¹¶ä¸ºä¸€ä¸ªè¾“å‡ºæ–‡ä»¶"""
    source_type = os.path.basename(input_folder)

    # åŠ¨æ€åŠ è½½ç¬¬ä¸‰é˜¶æ®µ prompt
    subject = extract_subject_from_source(source_type)

    # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
    output_dir = os.path.dirname(output_file)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_files = glob.glob(os.path.join(input_folder, "*.json"))
    
    # è¿‡æ»¤æ‰erræ–‡ä»¶
    json_files = [f for f in json_files if 'err' not in os.path.basename(f)]
    
    processed_count = 0
    total_files = len(json_files)
    
    # æ‰“å¼€è¾“å‡ºæ–‡ä»¶å‡†å¤‡å†™å…¥ï¼ˆJSON Linesæ ¼å¼ï¼‰
    with open(output_file, 'w', encoding='utf-8') as out_f:
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ–‡ä»¶
        with ThreadPoolExecutor(max_workers=min(max_workers, len(json_files))) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(process_single_file, file_path, subject): file_path 
                for file_path in json_files
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in tqdm(as_completed(future_to_file), total=total_files, desc="Processing files"):
                file_path = future_to_file[future]
                try:
                    results = future.result()
                    # å†™å…¥ç»“æœ
                    for result_line in results:
                        out_f.write(result_line + "\n")
                    processed_count += 1
                except Exception as e:
                    logging.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    print(f"\nå¤„ç†å®Œæˆï¼å·²å¤„ç† {processed_count} ä¸ªæ–‡ä»¶ã€‚")
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    return processed_count

# åŸè„šæœ¬å…¶ä»–éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œåªä¿®æ”¹å¾ªç¯å¤„ç†éƒ¨åˆ†

if __name__ == "__main__":
    # ä¿ç•™åŸè„šæœ¬çš„å‚æ•°å’Œè·¯å¾„è®¾ç½®
    root = sys.argv[1]
    batch = sys.argv[2]
    input_png_dir = f'{root}/{batch}/7_qa_filter_{batch}'
    output_parent_dir = f'{root}/{batch}/8_tidan_filter_{batch}'
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆæ— éœ€å†åˆ›å»ºå­æ–‡ä»¶å¤¹ï¼Œç›´æ¥åœ¨çˆ¶ç›®å½•ä¸‹ç”Ÿæˆä¸¤ä¸ªåˆå¹¶æ–‡ä»¶ï¼‰
    os.makedirs(output_parent_dir, exist_ok=True)
    
    # å®šä¹‰ä¸¤ä¸ªåˆå¹¶æ–‡ä»¶çš„è·¯å¾„
    normal_path = os.path.join(output_parent_dir, f'{batch}_llm_filterå¯ç”¨æ€§æ£€æŸ¥.json')
    only_q_path = os.path.join(output_parent_dir, f'{batch}only_q_llm_filterå¯ç”¨æ€§æ£€æŸ¥.json')
    merged_path = os.path.join(output_parent_dir, f'{batch}_llm_filterå¯ç”¨æ€§æ£€æŸ¥_qaå’Œonly_qåˆå¹¶ç‰ˆ.json')

    # åˆå§‹åŒ–ä¸‰ä¸ªæ–‡ä»¶ï¼ˆæ¸…ç©ºæˆ–åˆ›å»ºï¼‰
    for path in [normal_path, only_q_path, merged_path]:
        with open(path, 'w', encoding='utf-8') as f:
            pass

    # åˆå§‹åŒ–è®¡æ•°å™¨
    count_normal = 0  # æœ‰ç­”æ¡ˆçš„
    count_only_q = 0  # æ— ç­”æ¡ˆçš„

    # è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹
    sub_folders = [d for d in os.listdir(input_png_dir)
                   if os.path.isdir(os.path.join(input_png_dir, d))]

    if not sub_folders:
        print(f"åœ¨ç›®å½• {input_png_dir} ä¸­æœªæ‰¾åˆ°å­æ–‡ä»¶å¤¹")
        sys.exit(1)

    print(f"è¾“å…¥ç›®å½•: {input_png_dir}")
    print(f"è¾“å‡ºæ–‡ä»¶:")
    print(f"  1. qa: {normal_path}")
    print(f"  2. only_q: {only_q_path}")
    print(f"  3. merged: {merged_path}")
    print(f"æ‰¾åˆ° {len(sub_folders)} ä¸ªå­æ–‡ä»¶å¤¹ï¼Œå¼€å§‹å¤„ç†...")

    # æ ¹æ®CPUæ ¸å¿ƒæ•°ç¡®å®šæœ€å¤§å¹¶å‘æ•°
    max_workers = min(len(sub_folders), mp.cpu_count())

    for sub_folder in sub_folders:
        if 'err' in sub_folder:
            continue
            
        subfolder_path = os.path.join(input_png_dir, sub_folder)
        # ç”Ÿæˆä¸´æ—¶è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¤„ç†å®Œæˆåä¼šåˆå¹¶ï¼‰
        temp_output = os.path.join(output_parent_dir, f'temp_{sub_folder}_llm_filter_{batch.replace(".", "")}.json')
        
        # è°ƒç”¨åŸå¤„ç†å‡½æ•°ç”Ÿæˆå•ä¸ªå­æ–‡ä»¶å¤¹çš„JSONç»“æœ
        processed_count = process_folder(subfolder_path, temp_output, max_workers)
        
        # åˆ¤æ–­æ˜¯å¦ä¸º only_q ç±»å‹
        is_only_q = 'only_q' in sub_folder
        target_path = only_q_path if is_only_q else normal_path

        line_count = 0
        # è¯»å–ä¸´æ—¶æ–‡ä»¶å¹¶å†™å…¥å¯¹åº”æ–‡ä»¶ + åˆå¹¶æ–‡ä»¶
        try:
            with open(temp_output, 'r', encoding='utf-8') as temp_f:
                lines = [line for line in temp_f if line.strip()]
        except Exception as e:
            logging.error(f"è¯»å–ä¸´æ—¶æ–‡ä»¶ {temp_output} æ—¶å‡ºé”™: {e}")
            lines = []

        # å†™å…¥å¯¹åº”ç±»å‹æ–‡ä»¶
        try:
            with open(target_path, 'a', encoding='utf-8') as type_f:
                for line in lines:
                    type_f.write(line)
        except Exception as e:
            logging.error(f"å†™å…¥ç›®æ ‡æ–‡ä»¶ {target_path} æ—¶å‡ºé”™: {e}")

        # å†™å…¥åˆå¹¶æ–‡ä»¶
        try:
            with open(merged_path, 'a', encoding='utf-8') as merged_f:
                for line in lines:
                    merged_f.write(line)
        except Exception as e:
            logging.error(f"å†™å…¥åˆå¹¶æ–‡ä»¶ {merged_path} æ—¶å‡ºé”™: {e}")

        # æ›´æ–°è®¡æ•°
        line_count = len(lines)
        if is_only_q:
            count_only_q += line_count
            print(f"âœ… [only_q]  {sub_folder} -> {os.path.basename(target_path)} (+{line_count}è¡Œ)")
        else:
            count_normal += line_count
            print(f"âœ… [normal]  {sub_folder} -> {os.path.basename(target_path)} (+{line_count}è¡Œ)")

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(temp_output)
        except Exception as e:
            logging.error(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {temp_output} æ—¶å‡ºé”™: {e}")

    # æœ€ç»ˆç»Ÿè®¡
    total = count_normal + count_only_q
    print("\n" + "="*60)
    print("ğŸ“Š å¤„ç†å®Œæˆï¼è¾“å‡ºä¸‰ä¸ªæ–‡ä»¶ï¼š")
    print(f"   1. æœ‰ç­”æ¡ˆ (normal): {count_normal} æ¡ -> {os.path.basename(normal_path)}")
    print(f"   2. æ²¡æœ‰ç­”æ¡ˆ (only_q): {count_only_q} æ¡ -> {os.path.basename(only_q_path)}")
    print(f"   3. åˆå¹¶æ€»è®¡: {total} æ¡ -> {os.path.basename(merged_path)}")